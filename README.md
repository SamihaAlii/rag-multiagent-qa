RAG Local Agent Demo

This project demonstrates a simple Retrieval-Augmented Generation (RAG) pipeline using local language models and LangChain. It retrieves relevant information from local text documents and answers user questions via a basic agentic workflow.

---

## ğŸ“Œ Features

- âœ… Document ingestion from `.txt` files
- âœ… Vector store creation using FAISS
- âœ… Keyword-based agent routing to tools
- âœ… Streamlit-based interactive UI
- âœ… Completely offline (no OpenAI or cloud dependencies)
- âœ… Lightweight models run locally via Hugging Face

---

## ğŸ—‚ï¸ Project Structure

rag_local_agent_demo/
â”œâ”€â”€ docs/ # Contains source .txt documents
â”‚ â”œâ”€â”€ support_info.txt
â”‚ â”œâ”€â”€ product_specs.txt
â”‚ â””â”€â”€ return_policy.txt
â”œâ”€â”€ rag_pipeline_demo.py # Main Streamlit app
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Project overview and usage

yaml
Copy
Edit

---

## âš™ï¸ Installation

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/rag_local_agent_demo.git
cd rag_local_agent_demo
2. Create virtual environment (optional but recommended)
bash
Copy
Edit
python -m venv venv
source venv/bin/activate   # On Windows: venv\Scripts\activate
3. Install dependencies
bash
Copy
Edit
pip install -r requirements.txt
ğŸš€ Running the Streamlit App
bash
Copy
Edit
streamlit run rag_pipeline_demo.py
The app will:

Load and embed your documents from the docs/ folder.

Launch a simple UI for asking questions.

Use a keyword-based agent to route queries to the right context.

ğŸ“„ Example Query
Input
What are your customer support hours?

Output
Our support team is available Monday through Friday, 9 AM to 5 PM IST.

